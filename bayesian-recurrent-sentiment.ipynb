{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    string = re.sub('[^A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    length = len(string)\n",
    "    string = ' '.join(string)\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('rt-polarity.neg', 'r') as fopen:\n",
    "    negatives = fopen.read().split('\\n')\n",
    "with open('rt-polarity.pos', 'r') as fopen:\n",
    "    positives = fopen.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negatives = negatives[:50]\n",
    "positives = positives[:50]\n",
    "\n",
    "for i in range(len(negatives)):\n",
    "    negatives[i] = clearstring(negatives[i])\n",
    "    positives[i] = clearstring(positives[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 926\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for i in range(len(negatives)):\n",
    "    vocab += negatives[i].split()\n",
    "    vocab += positives[i].split()\n",
    "    \n",
    "vocab = sorted(vocab, key = vocab.count,reverse = True)\n",
    "d1 = dict((k,v) for v,k in enumerate(reversed(vocab)))\n",
    "vocab = ['PAD', 'EOS'] + sorted(d1, key = d1.get, reverse = True)\n",
    "print('vocab size:', len(vocab))\n",
    "dictionary = dict(zip(vocab, [i for i in range(len(vocab))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = negatives + positives\n",
    "y_data = [0] * len(negatives) + [1] * len(positives)\n",
    "onehot = np.zeros((len(negatives) + len(positives), 2))\n",
    "for i in range(onehot.shape[0]):\n",
    "    onehot[i, y_data[i]] = 1.0\n",
    "    \n",
    "x_train, x_test, y_train, y_test, y_train_label, y_test_label = train_test_split(x_data, onehot, y_data, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Activation function:\n",
    "0- for sigmoid\n",
    "1- for tanh\n",
    "2- for relu\n",
    "\n",
    "Now the constants are:\n",
    "1- batch size : 20\n",
    "2- epoch: 50\n",
    "3- gradient descent\n",
    "4- softmax with cross entropy\n",
    "```\n",
    "\n",
    "So you can change anything you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network(num_hidden, size_layer, learning_rate, dropout_rate, beta, activation, seq_len,\n",
    "                   batch_size = 20):\n",
    "    tf.reset_default_graph()\n",
    "    def lstm_cell(size_layer, activation):\n",
    "        if activation == 0:\n",
    "            activation = tf.nn.sigmoid\n",
    "        elif activation == 1:\n",
    "            activation = tf.nn.tanh\n",
    "        else:\n",
    "            activation = tf.nn.relu\n",
    "        return tf.nn.rnn_cell.LSTMCell(size_layer, activation = activation)\n",
    "    rnn_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(size_layer, activation) for _ in range(num_hidden)])\n",
    "    X = tf.placeholder(tf.float32, [None, None, 1])\n",
    "    Y = tf.placeholder(tf.float32, [None, np.unique(y_train).shape[0]])\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(rnn_cells, output_keep_prob = dropout_rate)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(drop, X, dtype = tf.float32)\n",
    "    rnn_W = tf.Variable(tf.random_normal((size_layer, np.unique(y_train).shape[0])))\n",
    "    rnn_B = tf.Variable(tf.random_normal([np.unique(y_train).shape[0]]))\n",
    "    logits = tf.matmul(outputs[:, -1], rnn_W) + rnn_B\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "    cost += sum(beta * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    COST, TEST_COST, ACC, TEST_ACC = [], [], [], []\n",
    "    \n",
    "    for i in range(50):\n",
    "        train_acc, train_loss = 0, 0\n",
    "        for n in range(0, (len(x_train) // batch_size) * batch_size, batch_size):\n",
    "            batch_x = np.zeros((batch_size, seq_len, 1))\n",
    "            for k in range(batch_size):\n",
    "                tokens = x_train[n + k].split()[:seq_len]\n",
    "                for no, text in enumerate(tokens[::-1]):\n",
    "                    try:\n",
    "                        batch_x[k, -1 - no, 0] = dictionary[text]\n",
    "                    except:\n",
    "                        batch_x[k, -1 - no, 0] = -1\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict = {X: batch_x, Y: y_train[n: n + batch_size, :]})\n",
    "            train_acc += sess.run(accuracy, feed_dict = {X: batch_x, Y: y_train[n: n + batch_size, :]})\n",
    "            train_loss += loss\n",
    "            \n",
    "        batch_x = np.zeros((len(x_test), seq_len, 1))\n",
    "        for k in range(len(x_test)):\n",
    "            tokens = x_test[k].split()[:seq_len]\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    batch_x[k, -1 - no, 0] = dictionary[text]\n",
    "                except:\n",
    "                    batch_x[k, -1 - no, 0] = -1\n",
    "        results = sess.run([cost, accuracy], feed_dict = {X: batch_x, Y: y_test})\n",
    "        TEST_COST.append(results[0])\n",
    "        TEST_ACC.append(results[1])\n",
    "        train_loss /= (len(x_train) // batch_size)\n",
    "        train_acc /= (len(x_train) // batch_size)\n",
    "        ACC.append(train_acc)\n",
    "        COST.append(train_loss)\n",
    "    COST = np.array(COST).mean()\n",
    "    TEST_COST = np.array(TEST_COST).mean()\n",
    "    ACC = np.array(ACC).mean()\n",
    "    TEST_ACC = np.array(TEST_ACC).mean()\n",
    "    return COST, TEST_COST, ACC, TEST_ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_nn(num_hidden, size_layer, learning_rate, dropout_rate, beta, activation, seq_len):\n",
    "    global accbest\n",
    "    param = {\n",
    "        'num_hidden' : int(np.around(num_hidden)),\n",
    "        'size_layer' : int(np.around(size_layer)),\n",
    "        'learning_rate' : max(min(learning_rate, 1), 0.0001),\n",
    "        'dropout_rate' : max(min(dropout_rate, 0.99), 0),\n",
    "        'beta' : max(min(beta, 0.5), 0.000001),\n",
    "        'activation': int(np.around(activation)),\n",
    "        'seq_len' : int(np.around(seq_len))\n",
    "    }\n",
    "    print(\"\\nSearch parameters %s\" % (param), file = log_file)\n",
    "    log_file.flush()\n",
    "    learning_cost, valid_cost, learning_acc, valid_acc = neural_network(**param)\n",
    "    print(\"stop after 50 iteration with train cost %f, valid cost %f, train acc %f, valid acc %f\" % (learning_cost, valid_cost, learning_acc, valid_acc))\n",
    "    if (valid_acc > accbest):\n",
    "        costbest = valid_acc\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   activation |      beta |   dropout_rate |   learning_rate |   num_hidden |   seq_len |   size_layer | \n",
      "stop after 50 iteration with train cost 42.381874, valid cost 36.617153, train acc 0.546000, valid acc 0.500000\n",
      "    1 | 00m10s | \u001b[35m   0.50000\u001b[0m | \u001b[32m      0.6067\u001b[0m | \u001b[32m   0.3674\u001b[0m | \u001b[32m        0.5718\u001b[0m | \u001b[32m         0.6362\u001b[0m | \u001b[32m      2.9054\u001b[0m | \u001b[32m   6.3420\u001b[0m | \u001b[32m    435.3363\u001b[0m | \n",
      "stop after 50 iteration with train cost 3.349305, valid cost 2.959107, train acc 0.527750, valid acc 0.500000\n",
      "    2 | 00m04s |    0.50000 |       0.6907 |    0.2037 |         0.8793 |          0.1485 |       2.9604 |   14.1557 |      34.4793 | \n",
      "stop after 50 iteration with train cost 15.869358, valid cost 14.480788, train acc 0.546250, valid acc 0.500000\n",
      "    3 | 00m15s |    0.50000 |       1.1585 |    0.1007 |         0.6277 |          0.7885 |       7.2214 |   16.4103 |     235.6704 | \n",
      "stop after 50 iteration with train cost 4.302577, valid cost 3.017692, train acc 0.475000, valid acc 0.500000\n",
      "    4 | 00m10s |    0.50000 |       0.2487 |    0.0791 |         0.7831 |          0.9348 |       9.0891 |   13.8299 |      90.4191 | \n",
      "stop after 50 iteration with train cost nan, valid cost nan, train acc 0.500000, valid acc 0.500000\n",
      "    5 | 00m07s |    0.50000 |       1.9927 |    0.0077 |         0.6365 |          0.2179 |       5.1895 |    9.3360 |     264.7745 | \n",
      "stop after 50 iteration with train cost 29.283845, valid cost 25.389284, train acc 0.527500, valid acc 0.510000\n",
      "    6 | 00m07s | \u001b[35m   0.51000\u001b[0m | \u001b[32m      0.9636\u001b[0m | \u001b[32m   0.2779\u001b[0m | \u001b[32m        0.1064\u001b[0m | \u001b[32m         0.1390\u001b[0m | \u001b[32m      2.6789\u001b[0m | \u001b[32m  16.7739\u001b[0m | \u001b[32m    242.9170\u001b[0m | \n",
      "stop after 50 iteration with train cost nan, valid cost nan, train acc 0.500000, valid acc 0.500000\n",
      "    7 | 00m09s |    0.50000 |       1.5511 |    0.4615 |         0.1376 |          0.6660 |       3.1947 |   14.7922 |     401.3807 | \n",
      "stop after 50 iteration with train cost 387.484119, valid cost 358.366577, train acc 0.492250, valid acc 0.516000\n",
      "    8 | 00m26s | \u001b[35m   0.51600\u001b[0m | \u001b[32m      1.5552\u001b[0m | \u001b[32m   0.3468\u001b[0m | \u001b[32m        0.3169\u001b[0m | \u001b[32m         0.0444\u001b[0m | \u001b[32m      9.8702\u001b[0m | \u001b[32m   9.2395\u001b[0m | \u001b[32m    495.6505\u001b[0m | \n",
      "stop after 50 iteration with train cost 73.797551, valid cost 61.530197, train acc 0.531250, valid acc 0.495000\n",
      "    9 | 00m16s |    0.49500 |       0.7076 |    0.2841 |         0.6419 |          0.1419 |       5.0061 |   12.3230 |     491.8022 | \n",
      "stop after 50 iteration with train cost 15.092491, valid cost 14.541831, train acc 0.543500, valid acc 0.497000\n",
      "   10 | 00m07s |    0.49700 |       0.8510 |    0.0589 |         0.4873 |          0.7115 |       4.5997 |   11.0504 |     185.4424 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   activation |      beta |   dropout_rate |   learning_rate |   num_hidden |   seq_len |   size_layer | \n",
      "stop after 50 iteration with train cost 20.418870, valid cost 10.943871, train acc 0.548000, valid acc 0.504000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.19648828e-05]), 'funcalls': 60, 'warnflag': 2, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 6}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 | 00m36s |    0.50400 |       1.9722 |    0.1400 |         0.3632 |          0.9942 |       9.3480 |    5.1808 |     511.7820 | \n",
      "stop after 50 iteration with train cost 44.214597, valid cost 39.447975, train acc 0.547000, valid acc 0.511000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.09022457e-05]), 'funcalls': 49, 'warnflag': 2, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 4}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 00m26s |    0.51100 |       1.7500 |    0.0607 |         0.4002 |          0.3759 |       9.9711 |    5.0667 |     470.4412 | \n",
      "stop after 50 iteration with train cost 99.537190, valid cost 94.412079, train acc 0.523000, valid acc 0.493000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.78025875e-05]), 'funcalls': 59, 'warnflag': 2, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 7}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13 | 00m43s |    0.49300 |       0.3265 |    0.0916 |         0.4331 |          0.1130 |       9.9007 |   19.3773 |     326.0415 | \n",
      "stop after 50 iteration with train cost 274.715683, valid cost 274.448212, train acc 0.509500, valid acc 0.512000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.68241077e-05]), 'funcalls': 51, 'warnflag': 2, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 5}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 00m55s |    0.51200 |       1.4632 |    0.0478 |         0.1868 |          0.0046 |       9.9877 |   19.9527 |     436.0364 | \n",
      "stop after 50 iteration with train cost nan, valid cost nan, train acc 0.500000, valid acc 0.500000\n",
      "   15 | 00m12s |    0.50000 |       1.7405 |    0.4607 |         0.4442 |          0.3457 |       2.0989 |   19.9088 |     135.9072 | \n",
      "stop after 50 iteration with train cost 219.328286, valid cost 214.091675, train acc 0.494000, valid acc 0.493000\n",
      "   16 | 00m23s |    0.49300 |       1.7248 |    0.0845 |         0.1401 |          0.0570 |       9.8453 |    5.1397 |     422.3794 | \n",
      "stop after 50 iteration with train cost nan, valid cost nan, train acc 0.500250, valid acc 0.500000\n",
      "   17 | 00m10s |    0.50000 |       1.5464 |    0.0535 |         0.6583 |          0.2869 |       2.0762 |    5.1217 |      71.4014 | \n",
      "stop after 50 iteration with train cost 1.684258, valid cost 1.324773, train acc 0.547250, valid acc 0.502000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  1.05064401e-05]), 'funcalls': 48, 'warnflag': 2, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 5}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18 | 00m12s |    0.50200 |       0.5147 |    0.0981 |         0.2503 |          0.8700 |       2.1998 |   19.8795 |      64.4708 | \n",
      "stop after 50 iteration with train cost 20.364790, valid cost 16.596560, train acc 0.543500, valid acc 0.504000\n",
      "   19 | 00m14s |    0.50400 |       0.6509 |    0.1735 |         0.9538 |          0.3490 |       2.0384 |   19.8586 |     280.5687 | \n",
      "stop after 50 iteration with train cost 35.997788, valid cost 7.070683, train acc 0.550000, valid acc 0.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.40310661e-05]), 'funcalls': 51, 'warnflag': 2, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 5}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 00m26s |    0.50000 |       1.8937 |    0.4665 |         0.7281 |          0.5549 |       9.9243 |    5.0304 |     496.3627 | \n",
      "stop after 50 iteration with train cost 33.316251, valid cost 31.917385, train acc 0.532750, valid acc 0.498000\n",
      "   21 | 01m05s |    0.49800 |       1.3592 |    0.0136 |         0.4119 |          0.9045 |       9.9148 |   19.6690 |     511.7459 | \n",
      "stop after 50 iteration with train cost 335.914213, valid cost 330.742371, train acc 0.516500, valid acc 0.501000\n",
      "   22 | 00m59s |    0.50100 |       1.3139 |    0.0896 |         0.3180 |          0.0344 |       9.8874 |   19.1406 |     473.2550 | \n",
      "stop after 50 iteration with train cost 22.918857, valid cost 12.320593, train acc 0.549000, valid acc 0.498000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.76338156e-05]), 'funcalls': 46, 'warnflag': 2, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 3}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 | 00m18s |    0.49800 |       1.1204 |    0.4581 |         0.2750 |          0.3324 |       2.1985 |   19.9052 |     366.9115 | \n",
      "stop after 50 iteration with train cost 197.230115, valid cost 196.958145, train acc 0.495500, valid acc 0.485000\n",
      "   24 | 00m11s |    0.48500 |       0.0213 |    0.2907 |         0.2731 |          0.0010 |       2.0656 |    5.0754 |     219.8795 | \n",
      "stop after 50 iteration with train cost 518.450370, valid cost 490.195129, train acc 0.506000, valid acc 0.496000\n",
      "   25 | 00m34s |    0.49600 |       0.5991 |    0.3734 |         0.1730 |          0.0297 |       9.8292 |    9.3370 |     448.6081 | \n",
      "stop after 50 iteration with train cost 25.127167, valid cost 17.650429, train acc 0.530250, valid acc 0.490000\n",
      "   26 | 00m18s |    0.49000 |       0.1876 |    0.3141 |         0.1996 |          0.2209 |       9.9328 |    5.5145 |     149.1066 | \n",
      "stop after 50 iteration with train cost 65.672021, valid cost 63.703278, train acc 0.522500, valid acc 0.554000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.13953033e-05]), 'funcalls': 56, 'warnflag': 2, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 5}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   27 | 00m11s | \u001b[35m   0.55400\u001b[0m | \u001b[32m      1.1016\u001b[0m | \u001b[32m   0.0992\u001b[0m | \u001b[32m        0.1672\u001b[0m | \u001b[32m         0.0496\u001b[0m | \u001b[32m      2.3178\u001b[0m | \u001b[32m  19.8444\u001b[0m | \u001b[32m    420.9660\u001b[0m | \n",
      "stop after 50 iteration with train cost nan, valid cost nan, train acc 0.500000, valid acc 0.500000\n",
      "   28 | 00m34s |    0.50000 |       1.8591 |    0.3056 |         0.1494 |          0.1084 |       2.3779 |   19.4666 |     423.9416 | \n",
      "stop after 50 iteration with train cost 2.057963, valid cost 1.968167, train acc 0.488750, valid acc 0.498000\n",
      "   29 | 01m10s |    0.49800 |       0.0000 |    0.0000 |         0.9900 |          0.0001 |      10.0000 |   20.0000 |     413.4489 | \n",
      "stop after 50 iteration with train cost 14.022513, valid cost 13.733238, train acc 0.496250, valid acc 0.517000\n",
      "   30 | 00m33s |    0.51700 |       0.0000 |    0.0000 |         0.1000 |          0.0001 |       2.0000 |   20.0000 |     415.8341 | \n"
     ]
    }
   ],
   "source": [
    "log_file = open('nn-bayesian.log', 'a')\n",
    "accbest = 0.0\n",
    "NN_BAYESIAN = BayesianOptimization(generate_nn, \n",
    "                              {'num_hidden': (2, 10),\n",
    "                               'size_layer': (32, 512),\n",
    "                               'learning_rate': (0.0001, 1),\n",
    "                               'dropout_rate': (0.1, 0.99),\n",
    "                               'beta': (0.000001, 0.49),\n",
    "                               'activation': (0, 2),\n",
    "                               'seq_len': (5, 20)\n",
    "                              })\n",
    "NN_BAYESIAN.maximize(init_points = 10, n_iter = 20, acq = 'ei', xi = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum NN accuracy value: 0.554000\n",
      "Best NN parameters:  {'num_hidden': 2.3178396780664174, 'dropout_rate': 0.16724952060867815, 'beta': 0.099189911765081795, 'learning_rate': 0.049638440024850142, 'size_layer': 420.96604492562358, 'activation': 1.1015571286131713, 'seq_len': 19.844388160934063}\n"
     ]
    }
   ],
   "source": [
    "print('Maximum NN accuracy value: %f' % NN_BAYESIAN.res['max']['max_val'])\n",
    "print('Best NN parameters: ', NN_BAYESIAN.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means, best optimized parameters are:\n",
    "```text\n",
    "num hidden: 2\n",
    "dropout rate: 0.16724952060867815\n",
    "beta: 0.099189911765081795\n",
    "learning rate: 0.049638440024850142\n",
    "size layer: 421\n",
    "activation: tanh\n",
    "sequence length: 20\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
