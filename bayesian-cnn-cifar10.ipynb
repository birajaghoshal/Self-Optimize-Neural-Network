{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use Python 2. Python 3 got Cpickle problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def reshape_image(img):\n",
    "    img = img.reshape([3, 32, 32])\n",
    "    return img.transpose([1, 2, 0])\n",
    "\n",
    "unique_name = unpickle('/home/husein/space/cifar/cifar-10-batches-py/batches.meta')['label_names']\n",
    "cifar10 = unpickle('/home/husein/space/cifar/cifar-10-batches-py/data_batch_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = cifar10['data'][:300, :]\n",
    "y_data = cifar10['labels'][:300]\n",
    "onehot = np.zeros((x_data.shape[0], len(unique_name)))\n",
    "for i in range(x_data.shape[0]):\n",
    "    onehot[i, y_data[i]] = 1.0\n",
    "    \n",
    "x_train, x_test, y_train, y_test, y_train_label, y_test_label = train_test_split(x_data, onehot, y_data, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Activation function:\n",
    "0- for sigmoid\n",
    "1- for tanh\n",
    "2- for relu\n",
    "\n",
    "Now the constants are:\n",
    "1- batch size : 10\n",
    "2- epoch: 20\n",
    "3- adaptive gradient descent\n",
    "4- softmax with cross entropy\n",
    "5- 2 fully connected layers\n",
    "```\n",
    "\n",
    "So you can change anything you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network(fully_conn_size, len_layer_conv, kernel_size, learning_rate, pooling_size, multiply,\n",
    "                   dropout_rate, beta, activation, batch_normalization, batch_size = 10):\n",
    "    \n",
    "    if activation == 0:\n",
    "        activation = tf.nn.sigmoid\n",
    "    elif activation == 1:\n",
    "        activation = tf.nn.tanh\n",
    "    else:\n",
    "        activation = tf.nn.relu\n",
    "    \n",
    "    def conv_layer(x, conv, out_shape):\n",
    "        w = tf.Variable(tf.truncated_normal([conv, conv, int(x.shape[3]), out_shape]))\n",
    "        b = tf.Variable(tf.truncated_normal([out_shape], stddev = 0.01))\n",
    "        return tf.nn.conv2d(x, w, [1, 1, 1, 1], padding = 'SAME') + b\n",
    "    \n",
    "    def fully_connected(x, out_shape):\n",
    "        w = tf.Variable(tf.truncated_normal([int(x.shape[1]), out_shape]))\n",
    "        b = tf.Variable(tf.truncated_normal([out_shape], stddev = 0.01))\n",
    "        return tf.matmul(x, w) + b\n",
    "    \n",
    "    def pooling(x, k = 2, stride = 2):\n",
    "        return tf.nn.max_pool(x, ksize = [1, k, k, 1], strides = [1, stride, stride, 1], padding = 'SAME')\n",
    "        \n",
    "    X = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "    Y = tf.placeholder(tf.float32, (None, len(unique_name)))\n",
    "    train = tf.placeholder(tf.bool)\n",
    "    for i in range(len_layer_conv):\n",
    "        if i == 0:\n",
    "            conv = activation(conv_layer(X, kernel_size, int(X.shape[3]) * multiply))\n",
    "        else:\n",
    "            conv = activation(conv_layer(X, kernel_size, int(conv.shape[3]) * multiply))\n",
    "        conv = pooling(conv, k = pooling_size, stride = pooling_size)\n",
    "        if batch_normalization:\n",
    "            conv = tf.layers.batch_normalization(conv, training = train)\n",
    "        conv = tf.nn.dropout(conv, dropout_rate)\n",
    "    output_shape = int(conv.shape[1]) * int(conv.shape[2]) * int(conv.shape[3])\n",
    "    conv = tf.reshape(conv, [-1, output_shape])\n",
    "    for i in range(2):\n",
    "        fc = activation(fully_connected(conv, fully_conn_size))\n",
    "        fc = tf.nn.dropout(fc, dropout_rate)\n",
    "        if batch_normalization:\n",
    "            fc = tf.layers.batch_normalization(fc, training = train)\n",
    "    logits = fully_connected(fc, len(unique_name))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits))\n",
    "    cost += sum(beta * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    COST, TEST_COST, ACC, TEST_ACC = [], [], [], []\n",
    "    for i in range(20):\n",
    "        train_acc, train_loss = 0, 0\n",
    "        for n in range(0, (x_train.shape[0] // batch_size) * batch_size, batch_size):\n",
    "            batch_x = np.zeros((batch_size, 32, 32, 3))\n",
    "            for k in range(batch_size):\n",
    "                batch_x[k, :, :, :] = reshape_image(x_train[n + k, :])\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict = {X: batch_x, Y: y_train[n: n + batch_size, :], train: True})\n",
    "            train_acc += sess.run(accuracy, feed_dict = {X: batch_x, Y: y_train[n: n + batch_size, :], train: False})\n",
    "            train_loss += loss\n",
    "        batch_x = np.zeros((x_test.shape[0], 32, 32, 3))\n",
    "        for k in range(x_test.shape[0]):\n",
    "            batch_x[k, :, :, :] = reshape_image(x_test[k, :])\n",
    "        results = sess.run([cost, accuracy], feed_dict = {X: batch_x, Y: y_test, train: False})\n",
    "        TEST_COST.append(results[0])\n",
    "        TEST_ACC.append(results[1])\n",
    "        train_loss /= (x_train.shape[0] // batch_size)\n",
    "        train_acc /= (x_train.shape[0] // batch_size)\n",
    "        ACC.append(train_acc)\n",
    "        COST.append(train_loss)\n",
    "    COST = np.array(COST).mean()\n",
    "    TEST_COST = np.array(TEST_COST).mean()\n",
    "    ACC = np.array(ACC).mean()\n",
    "    TEST_ACC = np.array(TEST_ACC).mean()\n",
    "    return COST, TEST_COST, ACC, TEST_ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_nn(fully_conn_size, len_layer_conv, kernel_size, learning_rate, pooling_size, multiply,\n",
    "                dropout_rate, beta, activation, batch_normalization):\n",
    "    global accbest\n",
    "    param = {\n",
    "        'fully_conn_size' : int(np.around(fully_conn_size)),\n",
    "        'len_layer_conv' : int(np.around(len_layer_conv)),\n",
    "        'kernel_size': int(np.around(kernel_size)),\n",
    "        'learning_rate' : max(min(learning_rate, 1), 0.0001),\n",
    "        'pooling_size': int(np.around(pooling_size)),\n",
    "        'multiply': int(np.around(multiply)),\n",
    "        'dropout_rate' : max(min(dropout_rate, 0.99), 0),\n",
    "        'beta' : max(min(beta, 0.5), 0.000001),\n",
    "        'activation': int(np.around(activation)),\n",
    "        'batch_normalization' : int(np.around(batch_normalization))\n",
    "    }\n",
    "    learning_cost, valid_cost, learning_acc, valid_acc = neural_network(**param)\n",
    "    print(\"stop after 20 iteration with train cost %f, valid cost %f, train acc %f, valid acc %f\" % (learning_cost, valid_cost, learning_acc, valid_acc))\n",
    "    if (valid_acc > accbest):\n",
    "        costbest = valid_acc\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   activation |   batch_normalization |      beta |   dropout_rate |   fully_conn_size |   kernel_size |   learning_rate |   len_layer_conv |   multiply |   pooling_size | \n",
      "stop after 20 iteration with train cost 3045.623370, valid cost 850.600220, train acc 0.207708, valid acc 0.070000\n",
      "    1 | 00m11s | \u001b[35m   0.07000\u001b[0m | \u001b[32m      1.1727\u001b[0m | \u001b[32m               0.5934\u001b[0m | \u001b[32m   0.0486\u001b[0m | \u001b[32m        0.5660\u001b[0m | \u001b[32m          98.5965\u001b[0m | \u001b[32m       2.0852\u001b[0m | \u001b[32m         0.8936\u001b[0m | \u001b[32m          4.2865\u001b[0m | \u001b[32m    2.6944\u001b[0m | \u001b[32m        2.8386\u001b[0m | \n",
      "stop after 20 iteration with train cost 36739.563578, valid cost 3306.932373, train acc 0.169583, valid acc 0.066667\n",
      "    2 | 00m48s |    0.06667 |       0.2832 |                0.7160 |    0.2577 |         0.4641 |           67.9576 |        2.5873 |          0.2846 |           6.1693 |     2.5857 |         3.8789 | \n",
      "stop after 20 iteration with train cost 21869.972671, valid cost 24.594282, train acc 0.262292, valid acc 0.100000\n",
      "    3 | 00m07s | \u001b[35m   0.10000\u001b[0m | \u001b[32m      0.4940\u001b[0m | \u001b[32m               0.4563\u001b[0m | \u001b[32m   0.3368\u001b[0m | \u001b[32m        0.6253\u001b[0m | \u001b[32m          59.2714\u001b[0m | \u001b[32m       2.4663\u001b[0m | \u001b[32m         0.5029\u001b[0m | \u001b[32m          7.6350\u001b[0m | \u001b[32m    1.2304\u001b[0m | \u001b[32m        3.6642\u001b[0m | \n",
      "stop after 20 iteration with train cost 8412.721797, valid cost 8319.846680, train acc 0.207500, valid acc 0.102500\n",
      "    4 | 01m10s | \u001b[35m   0.10250\u001b[0m | \u001b[32m      1.0607\u001b[0m | \u001b[32m               0.2979\u001b[0m | \u001b[32m   0.0005\u001b[0m | \u001b[32m        0.6734\u001b[0m | \u001b[32m          49.4437\u001b[0m | \u001b[32m       2.3068\u001b[0m | \u001b[32m         0.9289\u001b[0m | \u001b[32m          5.9768\u001b[0m | \u001b[32m    2.5807\u001b[0m | \u001b[32m        2.6803\u001b[0m | \n",
      "stop after 20 iteration with train cost 24707.563184, valid cost 3.499001, train acc 0.227917, valid acc 0.100000\n",
      "    5 | 00m14s |    0.10000 |       1.8884 |                0.9081 |    0.2396 |         0.6810 |           61.6321 |        6.5173 |          0.9380 |           7.9094 |     1.2774 |         2.1233 | \n",
      "stop after 20 iteration with train cost 30740.138443, valid cost 2.341099, train acc 0.257917, valid acc 0.100000\n",
      "    6 | 00m23s |    0.10000 |       1.3389 |                0.6316 |    0.4678 |         0.5064 |           76.9860 |        3.1006 |          0.9354 |           6.8573 |     1.7062 |         3.6219 | \n",
      "stop after 20 iteration with train cost 32024.296793, valid cost 25.546030, train acc 0.228542, valid acc 0.100000\n",
      "    7 | 00m43s |    0.10000 |       0.4140 |                0.6338 |    0.1787 |         0.9635 |           85.4886 |        6.9190 |          0.9832 |           7.8658 |     1.8660 |         2.5934 | \n",
      "stop after 20 iteration with train cost 255531.801408, valid cost 137518.531250, train acc 0.159167, valid acc 0.078333\n",
      "    8 | 00m22s |    0.07833 |       0.5070 |                0.5534 |    0.1969 |         0.8619 |           74.3143 |        3.1396 |          0.1145 |           4.7203 |     2.3077 |         3.9478 | \n",
      "stop after 20 iteration with train cost 144096.979996, valid cost 116752.109375, train acc 0.230833, valid acc 0.090833\n",
      "    9 | 00m23s |    0.09083 |       1.0808 |                0.2060 |    0.0394 |         0.5703 |           68.8121 |        6.2146 |          0.2064 |           4.9460 |     1.9077 |         3.0363 | \n",
      "stop after 20 iteration with train cost 45350.642741, valid cost 1672.206787, train acc 0.222917, valid acc 0.075000\n",
      "   10 | 00m32s |    0.07500 |       1.2183 |                0.2314 |    0.3291 |         0.2959 |           51.0152 |        3.2358 |          0.9149 |           6.2200 |     1.6781 |         2.2848 | \n",
      "stop after 20 iteration with train cost 39325.443256, valid cost 29.612036, train acc 0.228125, valid acc 0.100000\n",
      "   11 | 00m48s |    0.10000 |       0.4563 |                0.7818 |    0.1838 |         0.6283 |           39.4618 |        3.5193 |          0.9598 |           4.8840 |     2.5023 |         3.4261 | \n",
      "stop after 20 iteration with train cost 57353.553802, valid cost 1263.891357, train acc 0.230208, valid acc 0.081667\n",
      "   12 | 00m29s |    0.08167 |       1.4132 |                0.0977 |    0.4254 |         0.6334 |           98.7928 |        2.4189 |          0.7328 |           4.7959 |     2.2889 |         3.6080 | \n"
     ]
    }
   ],
   "source": [
    "accbest = 0.0\n",
    "NN_BAYESIAN = BayesianOptimization(generate_nn, \n",
    "                              {'fully_conn_size': (16, 128),\n",
    "                               'len_layer_conv': (4, 8),\n",
    "                               'kernel_size': (2, 7),\n",
    "                               'learning_rate': (0.0001, 1),\n",
    "                               'pooling_size': (2, 4),\n",
    "                               'multiply': (1, 3),\n",
    "                               'dropout_rate': (0.1, 0.99),\n",
    "                               'beta': (0.000001, 0.49),\n",
    "                               'activation': (0, 2),\n",
    "                               'batch_normalization': (0, 1)\n",
    "                              })\n",
    "NN_BAYESIAN.maximize(init_points = 20, n_iter = 40, acq = 'ei', xi = 0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
